"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[666],{687:e=>{e.exports=JSON.parse('{"archive":{"blogPosts":[{"id":"10thworkshop","metadata":{"permalink":"/diracx/blog/10thworkshop","source":"@site/blog/2024-04-08.md","title":"The 10th DIRAC Users\' Workshop: 19th-21st June 2024, Lyon (FR)","description":"Dear All,","date":"2024-04-08T00:00:00.000Z","tags":[{"inline":false,"label":"DIRAC","permalink":"/diracx/blog/tags/dirac","description":"DIRAC tag description"},{"inline":false,"label":"LHCb","permalink":"/diracx/blog/tags/lhcb","description":"LHCb tag description"}],"readingTime":0.45,"hasTruncateMarker":false,"authors":[{"name":"Federico Stagni","title":"CERN","key":"fstagni"},{"name":"Andrei Tsaregorodtsev","title":"CERN","key":"atsar"},{"name":"Vanessa Hamar","title":"CERN","key":"vhamar"}],"frontMatter":{"slug":"10thworkshop","title":"The 10th DIRAC Users\' Workshop: 19th-21st June 2024, Lyon (FR)","authors":["fstagni","atsar","vhamar"],"tags":["dirac","lhcb"]},"unlisted":false,"nextItem":{"title":"LHCb and DIRAC Strategy Towards the LHCb Upgrade","permalink":"/diracx/blog/welcome"}},"content":"Dear All,\\n\\nThe 10th DIRAC Users\' workshop will take place 19th-21st June 2024 in CC-IN2P3 (Lyon, France).\\n\\nThe workshop will be devoted to the information exchange between the Dirac(X) developers, service administrators and users. Reports on the Dirac(x) ongoing and planned developments, service management tools will be included into the agenda together with the users reporting their experience. You are welcome to suggest your items to the workshop agenda.\\n\\nThanks to the generous welcome from CC-IN2P3, the workshop fee is 0.\\n\\nFor all details, and (free) registration, please visit https://indico.cern.ch/e/duw10."},{"id":"welcome","metadata":{"permalink":"/diracx/blog/welcome","source":"@site/blog/2019-08-17.md","title":"LHCb and DIRAC Strategy Towards the LHCb Upgrade","description":"The DIRAC project is developing interware to build and operate distributed computing systems. It provides a development framework and a rich set of services for both Workload and Data Management tasks of large scientific communities. DIRAC is adopted by a growing number of collaborations, including LHCb, Belle2, CLIC, and CTA. The LHCb experiment will be upgraded during the second long LHC shutdown (2019-2020). At restart of data taking in Run 3, the instantaneous luminosity will increase by a factor of five. The LHCb computing model also need be upgraded. Oversimplifying, this translates into the need for significantly more computing power and resources, and more storage with respect to what LHCb uses right now. The DIRAC interware will keep being the tool to handle all of LHCb distributed computing resources. Within this contribution, we highlight the ongoing and planned efforts to ensure that DIRAC will be able to provide an optimal usage of its distributed computing resources. This contribution focuses on DIRAC plans for increasing the scalability of the overall system, taking in consideration that the main requirement is keeping a running system working. This requirement translates into the need of studies and developments within the current DIRAC architecture. We believe that scalability is about traffic growth, dataset growth, and maintainability: within this contribution we address all of them, showing the technical solutions we are adopting.","date":"2019-08-17T00:00:00.000Z","tags":[{"inline":false,"label":"DIRAC","permalink":"/diracx/blog/tags/dirac","description":"DIRAC tag description"},{"inline":false,"label":"LHCb","permalink":"/diracx/blog/tags/lhcb","description":"LHCb tag description"},{"inline":false,"label":"Paper","permalink":"/diracx/blog/tags/paper","description":"Paper tag description"}],"readingTime":1.145,"hasTruncateMarker":false,"authors":[{"name":"Federico Stagni","title":"CERN","key":"fstagni"},{"name":"Andrei Tsaregorodtsev","title":"CERN","key":"atsar"},{"name":"Christophe Haen","key":"chaen"},{"name":"Philippe Charpentier","key":"pchar"},{"name":"Zoltan Mathe","key":"zmathe"},{"name":"Wojciech Jan Krzemien","key":"wkrz"},{"name":"Vladimir Romanovski","key":"vroman"}],"frontMatter":{"slug":"welcome","title":"LHCb and DIRAC Strategy Towards the LHCb Upgrade","authors":["fstagni","atsar","chaen","pchar","zmathe","wkrz","vroman"],"tags":["dirac","lhcb","paper"]},"unlisted":false,"prevItem":{"title":"The 10th DIRAC Users\' Workshop: 19th-21st June 2024, Lyon (FR)","permalink":"/diracx/blog/10thworkshop"},"nextItem":{"title":"DIRAC\u2014The Distributed MC Production and Analysis for LHCb","permalink":"/diracx/blog/dirac-2004"}},"content":"The DIRAC project is developing interware to build and operate distributed computing systems. It provides a development framework and a rich set of services for both Workload and Data Management tasks of large scientific communities. DIRAC is adopted by a growing number of collaborations, including LHCb, Belle2, CLIC, and CTA. The LHCb experiment will be upgraded during the second long LHC shutdown (2019-2020). At restart of data taking in Run 3, the instantaneous luminosity will increase by a factor of five. The LHCb computing model also need be upgraded. Oversimplifying, this translates into the need for significantly more computing power and resources, and more storage with respect to what LHCb uses right now. The DIRAC interware will keep being the tool to handle all of LHCb distributed computing resources. Within this contribution, we highlight the ongoing and planned efforts to ensure that DIRAC will be able to provide an optimal usage of its distributed computing resources. This contribution focuses on DIRAC plans for increasing the scalability of the overall system, taking in consideration that the main requirement is keeping a running system working. This requirement translates into the need of studies and developments within the current DIRAC architecture. We believe that scalability is about traffic growth, dataset growth, and maintainability: within this contribution we address all of them, showing the technical solutions we are adopting.\\n\\nSee more at https://www.epj-conferences.org/articles/epjconf/abs/2019/19/epjconf_chep2018_03012/epjconf_chep2018_03012.html."},{"id":"dirac-2004","metadata":{"permalink":"/diracx/blog/dirac-2004","source":"@site/blog/2004-09-30.md","title":"DIRAC\u2014The Distributed MC Production and Analysis for LHCb","description":"DIRAC is the LHCb distributed computing grid infrastructure for Monte Carlo (MC) production and analysis. Its architecture is based on a set of distributed collaborating services. The service decomposition broadly follows the CERN/ARDA-RTAG proposal, which should allow for the interchange of the EGEE/gLite and DIRAC components. In this paper we give an overview of the DIRAC architecture, as well as the main design choices in its implementation. The light nature and modular design of the DIRAC components allows its functionality to be easily extended to include new computing and storage elements or to handle new types of tasks. The DIRAC system already uses different types of computing resources - from single PC\'s to a variety of batch systems and to the Grid environment. In particular, the DIRAC interface to the LCG2 grid will be presented.","date":"2004-09-30T00:00:00.000Z","tags":[{"inline":false,"label":"DIRAC","permalink":"/diracx/blog/tags/dirac","description":"DIRAC tag description"},{"inline":false,"label":"LHCb","permalink":"/diracx/blog/tags/lhcb","description":"LHCb tag description"},{"inline":false,"label":"Paper","permalink":"/diracx/blog/tags/paper","description":"Paper tag description"}],"readingTime":0.69,"hasTruncateMarker":false,"authors":[{"name":"A Tsaregorodtsev","title":"Marseille, CPPM"},{"name":"P Charpentier","title":"CERN"},{"name":"M Frank","title":"CERN"},{"name":"V Garonne","title":"Marseille, CPPM"},{"name":"M Witek","title":"Cracow, INP"},{"name":"V Romanovski","title":"Serpukhov, IHEP"},{"name":"U Egede","title":"Imperial Coll., London"},{"name":"Vagnoni, V","title":"INFN, Bologna"},{"name":"Korolko, I","title":"Moscow, ITEP"},{"name":"Blouw, J","title":"(Heidelberg, Max Planck Inst.) ;"},{"name":"Kuznetsov, G","title":"(Rutherford)"},{"name":"Patrick, G","title":"(Rutherford)"},{"name":"Gandelman, M","title":"(U. Fed. Rio de Janeiro (main))"},{"name":"Graciani-Diaz, R","title":"(U. Barcelona (main))"},{"name":"Bernet, R","title":"(U. Zurich (main))"},{"name":"Brook, N","title":"(Bristol U.)"},{"name":"Pickford, A","title":"(Glasgow U.)"},{"name":"Tobin, M","title":"(U. Liverpool (main))"},{"name":"Saroka, A","title":"(U. Oxford (main))"},{"name":"Stokes-Rees, I","title":"(U. Oxford (main))"},{"name":"Saborido-Silva, J","title":"(U. Santiago de Compostela (main))"},{"name":"Sanchez-Garcia, M","title":"(U. Santiago de Compostela (main)) Hide"}],"frontMatter":{"slug":"dirac-2004","title":"DIRAC\u2014The Distributed MC Production and Analysis for LHCb","authors":[{"name":"A Tsaregorodtsev","title":"Marseille, CPPM"},{"name":"P Charpentier","title":"CERN"},{"name":"M Frank","title":"CERN"},{"name":"V Garonne","title":"Marseille, CPPM"},{"name":"M Witek","title":"Cracow, INP"},{"name":"V Romanovski","title":"Serpukhov, IHEP"},{"name":"U Egede","title":"Imperial Coll., London"},{"name":"Vagnoni, V","title":"INFN, Bologna"},{"name":"Korolko, I","title":"Moscow, ITEP"},{"name":"Blouw, J","title":"(Heidelberg, Max Planck Inst.) ;"},{"name":"Kuznetsov, G","title":"(Rutherford)"},{"name":"Patrick, G","title":"(Rutherford)"},{"name":"Gandelman, M","title":"(U. Fed. Rio de Janeiro (main))"},{"name":"Graciani-Diaz, R","title":"(U. Barcelona (main))"},{"name":"Bernet, R","title":"(U. Zurich (main))"},{"name":"Brook, N","title":"(Bristol U.)"},{"name":"Pickford, A","title":"(Glasgow U.)"},{"name":"Tobin, M","title":"(U. Liverpool (main))"},{"name":"Saroka, A","title":"(U. Oxford (main))"},{"name":"Stokes-Rees, I","title":"(U. Oxford (main))"},{"name":"Saborido-Silva, J","title":"(U. Santiago de Compostela (main))"},{"name":"Sanchez-Garcia, M","title":"(U. Santiago de Compostela (main)) Hide"}],"tags":["dirac","lhcb","paper"]},"unlisted":false,"prevItem":{"title":"LHCb and DIRAC Strategy Towards the LHCb Upgrade","permalink":"/diracx/blog/welcome"}},"content":"DIRAC is the LHCb distributed computing grid infrastructure for Monte Carlo (MC) production and analysis. Its architecture is based on a set of distributed collaborating services. The service decomposition broadly follows the CERN/ARDA-RTAG proposal, which should allow for the interchange of the EGEE/gLite and DIRAC components. In this paper we give an overview of the DIRAC architecture, as well as the main design choices in its implementation. The light nature and modular design of the DIRAC components allows its functionality to be easily extended to include new computing and storage elements or to handle new types of tasks. The DIRAC system already uses different types of computing resources - from single PC\'s to a variety of batch systems and to the Grid environment. In particular, the DIRAC interface to the LCG2 grid will be presented.\\n\\nMore details: https://cds.cern.ch/record/1494103"}]}}')}}]);